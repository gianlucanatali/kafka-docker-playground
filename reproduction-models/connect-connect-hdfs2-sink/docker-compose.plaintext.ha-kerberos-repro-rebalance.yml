---
version: '3.5'

services:

  krb5:
    image: flokkr/krb5
    hostname: krb5.kerberos-demo.local
    container_name: krb5
    volumes:
    - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/etc/krb5.conf
    # see fork https://github.com/vdesabou/issuer to avoid KrbException: Message stream modified (41) (https://stackoverflow.com/a/68212311/2381999)
    - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/issuer:/root/issuer

  namenode1:
    image: flokkr/hadoop:latest
    hostname: namenode1.kerberos-demo.local
    container_name: namenode1
    depends_on:
      - krb5
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-hadoop/dfs/name"
        SLEEP_SECONDS: 20
        # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    command: ["hdfs", "namenode"]
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  namenode2:
    image: flokkr/hadoop:latest
    hostname: namenode2.kerberos-demo.local
    container_name: namenode2
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    depends_on:
      - krb5
    environment:
        ENSURE_STANDBY_NAMENODE_DIR: "/tmp/hadoop-hadoop/dfs/name"
        #ENSURE_NAMENODE_CLUSTERID: "sh"
        SLEEP_SECONDS: 50
        # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    command: ["hdfs", "namenode"]
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  journal1:
    image: flokkr/hadoop:latest
    hostname: journal1.kerberos-demo.local
    container_name: journal1
    depends_on:
      - krb5
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    #environment:
        # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    command: ["hdfs", "journalnode"]
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  journal2:
    image: flokkr/hadoop:latest
    hostname: journal2.kerberos-demo.local
    container_name: journal2
    depends_on:
      - krb5
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    #environment:
        # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    command: ["hdfs", "journalnode"]
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  journal3:
    image: flokkr/hadoop:latest
    hostname: journal3.kerberos-demo.local
    container_name: journal3
    depends_on:
      - krb5
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    command: ["hdfs", "journalnode"]
    #environment:
         # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  datanode:
    image: flokkr/hadoop:latest
    command: ["hdfs", "datanode"]
    hostname: datanode.kerberos-demo.local
    container_name: datanode
    depends_on:
      - krb5
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    environment:
        SLEEP_SECONDS: 60
        # HADOOP_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug -Dsun.security.krb5.disableReferrals=true -Djdk.security.allowNonCaAnchor=true"
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  resourcemanager:
    image: flokkr/hadoop:latest
    hostname: resourcemanager.kerberos-demo.local
    container_name: resourcemanager
    depends_on:
      - krb5
    command: ["yarn", "resourcemanager"]
    env_file:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    volumes:
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/opt/launcher/plugins/011_kerberos/krb5.conf
        - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
        - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  activator:
    image: flokkr/hadoop:latest
    hostname: activator.kerberos-demo.local
    container_name: activator
    depends_on:
      - krb5
    command: kinit -kt /opt/hadoop/etc/hadoop/nn.keytab nn/activator.kerberos-demo.local;hdfs haadmin -transitionToActive nn1
    env_file:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/config
    environment:
        SLEEP_SECONDS: 70
    volumes:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/installer.sh:/opt/launcher/plugins/005_installer/installer.sh
      - ../../reproduction-models/connect-connect-hdfs2-sink/hadoop-2.7.4.tar.gz:/opt/download/hadoop.tar.gz

  connect:
    hostname: connect.kerberos-demo.local
    volumes:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/etc/krb5.conf
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    environment:
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components/confluentinc-kafka-connect-hdfs
      CONNECT_SCHEDULED_REBALANCE_MAX_DELAY_MS: 500

  connect2:
    # build:
    #   context: ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/
    #   args:
    #     TAG: ${TAG}
    #     CONNECTOR_TAG: ${CONNECTOR_TAG}
    image: vdesabou/kafka-docker-playground-connect:${CONNECT_TAG}
    volumes:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/etc/krb5.conf
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    hostname: connect2
    container_name: connect2
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - "18083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect2
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_PRODUCER_CLIENT_ID: "connect-worker-producer"
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      # Confluent Monitoring Interceptors for Control Center Streams Monitoring
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker:9092
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker:9092
      # CONNECT_LOG4J_ROOT_LOGLEVEL: DEBUG
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components/confluentinc-kafka-connect-hdfs
      # KAFKA_OPTS: -Dsun.security.krb5.debug=true
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_SCHEDULED_REBALANCE_MAX_DELAY_MS: 500

  connect3:
    # build:
    #   context: ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/
    #   args:
    #     TAG: ${TAG}
    #     CONNECTOR_TAG: ${CONNECTOR_TAG}
    image: vdesabou/kafka-docker-playground-connect:${CONNECT_TAG}
    volumes:
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/krb5.conf:/etc/krb5.conf
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ../../reproduction-models/connect-connect-hdfs2-sink/ha-kerberos/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    hostname: connect3
    container_name: connect3
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - "28083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect3
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_PRODUCER_CLIENT_ID: "connect-worker-producer"
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      # Confluent Monitoring Interceptors for Control Center Streams Monitoring
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker:9092
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker:9092
      # CONNECT_LOG4J_ROOT_LOGLEVEL: DEBUG
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components/confluentinc-kafka-connect-hdfs
      # KAFKA_OPTS: -Dsun.security.krb5.debug=true
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_SCHEDULED_REBALANCE_MAX_DELAY_MS: 500

networks:
  default:
    name: kerberos-demo.local